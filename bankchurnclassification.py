# -*- coding: utf-8 -*-
"""BankChurnClassification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_oeNzbwWryS6jJM1cdZSSjfN1bXebPTc

## Binary Classification with a Bank Churn Dataset
Objective of this model is to predict whether a customer continues with their account or closes it (e.g., churns).
"""

# setup
! mkdir ~/.kaggle
! cp kaggle.json ~/.kaggle/
! chmod 600 ~/.kaggle/kaggle.json

"""Install dataset from Kaggle"""

# download contest data
! kaggle competitions download -c playground-series-s4e1
! unzip playground-series-s4e1.zip

"""Import libraries"""

import pandas as pd
import numpy as np
from tensorflow import keras
from keras import layers
import matplotlib.pyplot as plt
from keras.optimizers import RMSprop
from keras.losses import BinaryCrossentropy
from keras.metrics import Accuracy
from sklearn.model_selection import KFold
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder, StandardScaler, MinMaxScaler

"""First, let's import training data. Before we continue, we perform some data pre-processing. In our data, we have columns of non-numerical data. We drop irrelevant ones and one-hot encode ones that do. We perform some basic feature selection, and drop columns that may not provide meaningful insight during training."""

data_path = './train.csv'
loaded_data = pd.read_csv(data_path)
loaded_data.head()

"""Our dataset is consists of non-numeric data, so we will need to perform some encodig and feature extraction"""

# Drop irrelevant columns (id, CustomerId, Surname)
loaded_data = loaded_data.drop(columns=["id", "CustomerId", "Surname"])

# one-hot encode categorical variables
loaded_data = pd.get_dummies(loaded_data, columns=["Geography", "Gender"])

# Seperate features and labels
loaded_labels = loaded_data["Exited"]
loaded_data = loaded_data.drop(columns=["Exited"])

loaded_data.head()

"""Given the dataset's size, we can allocate a training set, validation set, and a test set. We'll allocate these as follows:"""

# Split the data into training+validation and test sets
temp_data, test_data, temp_labels, test_labels = train_test_split(
    loaded_data, loaded_labels, test_size=0.2, random_state=42
)

# Further split the temporary data into training (80%) and validation (20%)
train_data, validation_data, train_labels, validation_labels = train_test_split(
    temp_data, temp_labels, test_size=0.2, random_state=42
)

train_data.shape

"""We now have tensors with numerial data. The range of each feature varys drastically, so perform scaling. We'll use a standard scaler to standardize the dataset by scaling its features so that they have the properties of a standard normal distribution. It's important that we are doing this after we have allocated different datasets."""

# Normalize/Standardize numerical features
scaler = StandardScaler()
train_data = scaler.fit_transform(train_data)
validation_data = scaler.fit_transform(validation_data)

"""With data scalled, we define our model:"""

input_shape = loaded_data.shape[1:]
input_shape

def build_model():
  inputs = layers.Input(shape=input_shape)
  hidden = layers.Dense(32, activation='relu')(inputs)
  hidden = layers.Dropout(0.2)(hidden)
  hidden = layers.Dense(16, activation='relu')(hidden)
  hidden = layers.Dropout(0.2)(hidden)
  outputs = layers.Dense(1, activation='sigmoid')(hidden)

  model = keras.models.Model(inputs=inputs, outputs=outputs, name="Bank_Churn_Model")

  model.compile(optimizer=RMSprop(),
                loss=BinaryCrossentropy(),
                metrics=['accuracy'])

  return model

model = build_model()
model.summary()

"""We've built this model with Keras' Functional API. We compile it with an `RMSprop` optimizer and `BinaryCrossentropy` loss function, as these are often used in binary classification tasks. With it defined, we begin training, passing in our validation dataset as well."""

history = model.fit(train_data,
                    train_labels,
                    epochs=10,
                    batch_size=256,
                    validation_data=(validation_data, validation_labels))

"""Finally, we can plot the model's performance"""

history_dict = history.history
loss_values = history_dict["loss"]
val_loss_values = history_dict["val_loss"]
epochs = range(1, len(loss_values) + 1)
plt.plot(epochs, loss_values, "bo", label="Training loss")
plt.plot(epochs, val_loss_values, "b", label="Validation loss")
plt.title("Training and validation loss")
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.legend()
plt.show()

"""Training further results in validation loss and training loss plateuing, stop training at 20 epochs. Next, we evaluate model performance on test-data set."""

test_data = scaler.fit_transform(test_data)

loss_of_model, accuracy_of_model = model.evaluate(test_data, test_labels)
print(loss_of_model)
print(accuracy_of_model)

"""We obtain an accuracy of `86%` and a loss value of `0.32`. Finally, let's make our predictions. We first process the data like before"""

test_data_path = './test.csv'
test_data = pd.read_csv(test_data_path)
test_id = test_data['id']

# Drop irrelevant columns (id, CustomerId, Surname)
test_data = test_data.drop(columns=["id", "CustomerId", "Surname"])

# one-hot encode categorical variables
test_data = pd.get_dummies(test_data, columns=["Geography", "Gender"])

test_data.head()

prediction = model.predict(test_data)
prediction = np.reshape(prediction, [prediction.shape[0]])
prediction.shape, test_id.shape

df = pd.DataFrame(data = {"id": test_id, "Exited": prediction})
df.head()

df.to_csv('submission.csv')